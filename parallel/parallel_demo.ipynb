{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import timm\n",
    "\n",
    "sys.path.append(\"../ML710-Project\")\n",
    "sys.path.append(\"../ML710-Project/model_and_hyperpam\")\n",
    "sys.path.append(\"../ML710-Project/dataset\")\n",
    "\n",
    "from dataset.classification_dataset import (\n",
    "    train_datasets,\n",
    ")\n",
    "\n",
    "from model_and_hyperpam import (\n",
    "    SEED,\n",
    "    BATCH_SIZE,\n",
    "    EPOCHS,\n",
    "    WANDB_API,\n",
    ")\n",
    "\n",
    "from parallel.model_parallel_class import ViTModelParallel\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError('CUDA is not available')\n",
    "\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "BATCH_SIZE = BATCH_SIZE //2\n",
    "EPOCHS = EPOCHS // 2\n",
    "\n",
    "model_base = timm.create_model('vit_large_patch16_224', pretrained=True, num_classes=5)\n",
    "\n",
    "device_ids = [0, 1]\n",
    "model_parallel = ViTModelParallel(model_base, device_ids)\n",
    "\n",
    "wandb.login(key=WANDB_API)\n",
    "run = wandb.init(project='ml710_project', entity='arcticfox', name='vit_model_parallel'+'_'+'_'+datetime.now().strftime('%Y%m%d_%H%M%S'), job_type=\"training\",reinit=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_datasets, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# test_dataloader = DataLoader(test_datasets, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epoch,device=f'cuda:{device_ids[0]}'):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for ep in tqdm(range(epoch)):\n",
    "        this_epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            target = target.to(output.device)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            this_epoch_loss += loss.item()\n",
    "        this_epoch_loss /= len(train_loader)\n",
    "        wandb.log({\"train_loss\": this_epoch_loss})\n",
    "    return model\n",
    "\n",
    "optimizer = optim.AdamW(model_parallel.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class ViTModelParallel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, device_ids):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        self.model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "\n",
    "        # 将部分层分配到不同的设备\n",
    "        self.model.patch_embed = self.model.patch_embed.to(device_ids[0])\n",
    "        self.model.pos_drop = self.model.pos_drop.to(device_ids[0])\n",
    "\n",
    "        self.model.blocks = nn.ModuleList(\n",
    "            [block.to(device_ids[i % len(device_ids)]) for i, block in enumerate(self.model.blocks)]\n",
    "        )\n",
    "\n",
    "        self.model.norm = self.model.norm.to(device_ids[-1])\n",
    "        self.model.head = self.model.head.to(device_ids[-1])\n",
    "        self.glob_pool = nn.AdaptiveAvgPool2d((1, 1)).to(self.device_ids[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device_ids[0])\n",
    "        x = self.model.patch_embed(x)\n",
    "        x = self.model.pos_drop(x)\n",
    "\n",
    "        for i, block in enumerate(self.model.blocks):\n",
    "            x = x.to(self.device_ids[i % len(self.device_ids)])\n",
    "            x = block(x)\n",
    "\n",
    "        x = x.to(self.device_ids[-1])\n",
    "        x = self.model.norm(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.model.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def main():\n",
    "    device_ids = ['cuda:0', 'cuda:1']  # 在这里设置要使用的GPU设备ID\n",
    "    model_name = 'vit_large_patch16_224'\n",
    "    num_classes = 5\n",
    "\n",
    "    model_parallel = ViTModelParallel(model_name, num_classes, device_ids)\n",
    "\n",
    "    # 测试模型\n",
    "    x = torch.randn(8, 3, 224, 224).to(device_ids[0])\n",
    "    output = model_parallel(x)\n",
    "    print(output.shape)\n",
    "\n",
    "if True:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "nn.parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device_ids = ['cuda:0', 'cuda:1']  # 在这里设置要使用的GPU设备ID\n",
    "    model_name = 'vit_large_patch16_224'\n",
    "    num_classes = 5\n",
    "\n",
    "    model_parallel = ViTModelParallel(model_name, num_classes, device_ids)\n",
    "\n",
    "    # 测试模型\n",
    "    x = torch.randn(8, 3, 224, 224).to(device_ids[0])\n",
    "    output = model_parallel(x)\n",
    "    print(output.shape)\n",
    "\n",
    "if True:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_parallel = timm.create_model('vit_large_patch16_224', pretrained=True, num_classes=5)\n",
    "device_ids = ['cuda:0', 'cuda:1']\n",
    "x = torch.randn(8, 3, 224, 224).to(device_ids[0])\n",
    "model_no_parallel = model_no_parallel.to(device_ids[0])\n",
    "output_no_parallel = model_no_parallel(x)\n",
    "print(output_no_parallel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_parallel.forward_features(x).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
